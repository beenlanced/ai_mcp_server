import asyncio
from contextlib import AsyncExitStack
import os
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv
from google import genai
from google.genai import types
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client


# Load Environment Variables (especially Gemini API key)
load_dotenv("../.env")

class MCPGeminiAIClient:
    """Client for interacting with GeminiAI models using MCP tools."""

    def __init__(self, model: str = "gemini-2.5-flash") -> None:
        """
        Initialize the GeminiAI MCP client

        Args:
            model (str, optional): The GeminiAI model to use. Defaults to "gemini-2.5-flash".
        """
        # Initialize session and client objects
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        self.gemini_client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
        self.model = model
        self.stdio: Optional[Any] = None
        self.write: Optional[Any] = None

    async def connect_to_server(self, server_script_path: str = "server.py") -> None:
        """
        Connect to an MCP server.

        Args:
            server_script_path (str): Path to the server script. Defaults to "server.py"
        """
        # Server configuration
        server_params = StdioServerParameters(
            command="python",
            args=[server_script_path],
        )

        # Connect to the server
        stdio_transport = await self.exit_stack.enter_async_context(
            stdio_client(server_params)
        )
        self.stdio, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(
            ClientSession(self.stdio, self.write)
        )

        # Initialize the connection
        await self.session.initialize()

        # List available tools
        tools_result = await self.session.list_tools()
        print("\nConnected to server with tools:")
        for tool in tools_result.tools:
            print(f"  - {tool.name}: {tool.description}")

    async def get_mcp_tools(self) -> List[Dict[str, Any]]:
        """
        Get available tools from the MCP server in Gemini AI format.

        Returns:
            List[Dict[str, Any]]: A list of tools in Gemini AI format.
        """
        tools_result = await self.session.list_tools()

        return [
            types.Tool(
                function_declarations=[
                    { 
                        "name": tool.name,
                        "description": tool.description,
                        "parameters": {
                            k: v
                            for k, v in tool.inputSchema.items()
                            if k not in ["additionalProperties", "$schema"]
                        },
                    }
                ]
            )
            for tool in tools_result.tools
        ]

    async def process_query(self, query: str) -> str:
        """
        Process a query using Gemini AI and available MCP tools.

        Args:
            query (str): The user query.

        Returns:
            str: The response from Gemini AI.
        """
        # Get available tools
        tools = await self.get_mcp_tools()

        # Initial GeminiAI API call - where expect to find tools
        response = self.gemini_client.models.generate_content(
            model=self.model,
            contents=query,
            config=types.GenerateContentConfig(
                temperature=0,
                tools=tools,
            ),
        )

        if response.candidates[0].content.parts[0].function_call:
            function_call = response.candidates[0].content.parts[0].function_call

            # Call the tool to get information 
            result = await self.session.call_tool(
                    function_call.name,
                    arguments = dict(function_call.args)
            )

            # Create a final query that appends our Tools data dump (i.e., the RAG content)
            query = query + result.content[0].text

            # Get final response from Gemini AI with tool results - to answer the original query
            final_response = self.gemini_client.models.generate_content(
                model=self.model,
                contents=query,
                config=types.GenerateContentConfig(
                    temperature=0,
                    tools=tools,
                )
            )

            return final_response.candidates[0].content.parts[0].text
        
        else:
             # No tool calls, just return the direct response
            print("No function call was generated by the model")
            return "No function call was generated"

    async def cleanup(self):
        """Clean up resources."""
        await self.exit_stack.aclose()

async def main():
    """Main entry point for the client."""
    client = MCPGeminiAIClient()
    await client.connect_to_server("server.py")

    # Example: Ask about company vacation policy
    query = "What is our company's vacation policy?"
    print(f"\nQuery: {query}")

    response = await client.process_query(query)
    print(f"\nAnswer - Response: {response}")

    #Need to make sure that I add this last step to close out resources with asyncio
    await client.cleanup()


if __name__ == "__main__":
    asyncio.run(main())